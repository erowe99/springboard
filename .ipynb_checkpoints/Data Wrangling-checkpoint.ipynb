{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis could be markdown later. Include something about the note book in here if necessary.\\n'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This could be markdown later. Include something about the note book in here if necessary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pendulum\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangling Section 1: Assessment Page Views.csv\n",
    "\"\"\"\n",
    "#Read in csv as dataframe\n",
    "#Using a smaller subset of the data since testing with 6mil+ rows would be too intensive\n",
    "df = pd.read_csv('/Users/ethanrowe/Dev/springboard_data/KSU Assessment Page Views Fall 2018.csv', nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate Outliers using the 1.5*interquartile rule\n",
    "IQR = float(df['duration'].quantile([0.75])) - float(df['duration'].quantile([0.25]))\n",
    "upper_bound = float(df['duration'].quantile([0.75])) + 1.5*IQR\n",
    "lower_bound = float(df['duration'].quantile([0.25])) - 1.5*IQR\n",
    "\n",
    "df_clean = df[(df['duration'] < upper_bound) & (df['duration'] > lower_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data:\n",
      "(8846, 13)\n",
      "assessment_attempt_id                                         0\n",
      "assessment_type                                               0\n",
      "canvas_assignment_id                                          0\n",
      "canvas_course_id                                              0\n",
      "canvas_section_id                                             0\n",
      "created_at                                                    0\n",
      "duration                                                      0\n",
      "id                                                            0\n",
      "load_time                                                     0\n",
      "page_id                                                       0\n",
      "raw_lti_param_ext_outcome_submission_submitted_at_accepted    0\n",
      "submit_time                                                   0\n",
      "user_param_external_user_id                                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned Data:\")\n",
    "print(df_clean.shape)\n",
    "print(df.isna().sum())\n",
    "#df_clean.to_csv('KSU Assessment Page Views Fall 2018 Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grouped by Assignment\n",
      "(580, 1)\n",
      "\n",
      "Grouped by User\n",
      "(1865, 1)\n"
     ]
    }
   ],
   "source": [
    "#Group by page and by students to compare average assignment durations\n",
    "assignment_df = pd.DataFrame(df_clean.groupby(['canvas_assignment_id'])['duration'].mean())\n",
    "print('\\nGrouped by Assignment')\n",
    "assignment_df.columns = ['Average Duration']\n",
    "print(page_df.shape)\n",
    "\n",
    "user_df = pd.DataFrame(df_clean.groupby(['user_param_external_user_id', 'canvas_assignment_id'])['duration'].mean())\n",
    "print('\\nGrouped by User')\n",
    "user_df.columns = ['Average Duration']\n",
    "print(user_df.shape)\n",
    "\n",
    "\n",
    "#page_df.to_csv('KSU Assessment Assignment Duration Averages.csv')\n",
    "#user_df.to_csv('KSU Assessment Student Views by User.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#User_df is good for visualizing the data, but a df that is easier to work with will be labeled sub_df. It has the \n",
    "#same columns as user_df, just in a different order with a numerical index. It will also be sorted by \n",
    "#canvas_assignment_id in order to match assignment_df's index\n",
    "sub_df = user_df.reset_index()  \n",
    "sub_df = sub_df.set_index('canvas_assignment_id')  \n",
    "sub_df = sub_df.sort_index()     \n",
    "sub_df = sub_df.reset_index()    \n",
    "\n",
    "#Using the user_df, turn it back into a dictionary grouped by canvas_assignment ids\n",
    "dfs = {}       \n",
    "for entry in sub_df.iterrows():  \n",
    "    if index != entry[1]['canvas_assignment_id']:  \n",
    "        index = entry[1]['canvas_assignment_id']  \n",
    "        df = pd.DataFrame(columns=['canvas_assignment_id', 'user_param_external_user_id', 'Average Duration'])  \n",
    "        df = df.append(dict(entry[1]), ignore_index = True) \n",
    "        dfs[index] = df  \n",
    "    else:  \n",
    "        dfs[index] = dfs[index].append(dict(entry[1]), ignore_index = True)  \n",
    "\n",
    "#Now page_df and this dictionary dfs should have one to one matches for the loop below\n",
    "#This loop will find all the student-assignment_id pairs that had an above average duration for that assignment\n",
    "above_avg_df = pd.DataFrame(columns = ['canvas_assignment_id', 'user_param_external_user_id','Average Duration']) \n",
    "for entry in assignment_df.iterrows(): \n",
    "    df = dfs[entry[0]] \n",
    "    above_avg_chunk = df[df['Average Duration'] >= entry[1][0]] \n",
    "    above_avg_df = above_avg_df.append(above_avg_chunk) \n",
    "#above_avg_df.to_csv('KSU Students with Above Average Assignment Duration.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students with longer than average assignmnet durations:\n",
      "(1018, 3)\n",
      "   canvas_assignment_id  user_param_external_user_id  Average Duration\n",
      "0               40876.0                      37827.0      12831.500000\n",
      "0               40878.0                      37949.0       9549.583333\n",
      "3               40878.0                      37251.0      10307.500000\n",
      "0               40879.0                      38399.0      11579.000000\n",
      "5               40879.0                      40394.0      16064.857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Students with longer than average assignmnet durations:\")\n",
    "print(above_avg_df.shape)\n",
    "print(above_avg_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangling Section 2: Course Grades \n",
    "Course Grades has been cleand by hand in excel. The details are outlined in Data Wrangling.md\n",
    "\"\"\"\n",
    "#Import data\n",
    "df2 = pd.read_csv('/Users/ethanrowe/Dev/springboard_data/Course Grades (Final Grade Only View) Fall 2018 - Course Grades (Final Grade Only View) Fall 2018.csv')\n",
    "login_times_clean = pd.read_csv('/Users/ethanrowe/Dev/springboard_data/anon_students_login_times_clean.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTo-Do\\nLoad in correct csv's, double check what I need for this\\nMerge them or something like that or augment login_times_clean with the necessary columns\\n\\nDo stuff on df2 that is necessary for cleaning like checking for missing values.\\n\\nDo stuff on login_times_clean such as breaking it up by times and counting stuff. Basically all of this has been \\ndone in excel already, but this will be more accurate and adaptable moving forward.\\n\\n\""
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The next step will be to merge login_times_clean and df2. df2 has all of the grade \n",
    "#information for certain students by name and id\n",
    "\n",
    "#I might need to use a different dataset since that one is not anonymous, check Course Final Grades or something later\n",
    "#login_times_clean is also not anonymous and it has the last access time for students by name and id\n",
    "\n",
    "#I want a dataframe that has students grades along with last access times. Then I can break that up into those who\n",
    "#were active before and after certain periods to get a good idea of the distribution.\n",
    "\n",
    "#All of the students in login_times_clean have final grades less than 70%. I can use df2 for a number of things on its\n",
    "#own but I also need it to put login_times_clean into perspective.\n",
    "\n",
    "#First step is to clean df2 if necessary. Then start prepping all of them. They should be EDA ready by the end of this.\n",
    "\n",
    "\"\"\"\n",
    "To-Do\n",
    "Load in correct csv's, double check what I need for this\n",
    "Merge them or something like that or augment login_times_clean with the necessary columns\n",
    "\n",
    "Do stuff on df2 that is necessary for cleaning like checking for missing values.\n",
    "\n",
    "Do stuff on login_times_clean such as breaking it up by times and counting stuff. Basically all of this has been \n",
    "done in excel already, but this will be more accurate and adaptable moving forward.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   student id  course id  course sis        section  section id  section sis  \\\n",
      "0       38651        626         NaN   WELL 1000/08        1165          NaN   \n",
      "1       37597        655         NaN  WELL 1000/W83        1221          NaN   \n",
      "2       35615        646         NaN   WELL 1000/38        1205          NaN   \n",
      "3       37982        633         NaN  WELL 1000/W65        1179          NaN   \n",
      "4       40291        623         NaN   WELL 1000/07        1164          NaN   \n",
      "\n",
      "        term  term id  term sis  current score enrollment state  \n",
      "0  Fall 2018       20       NaN          10.06           active  \n",
      "1  Fall 2018       20       NaN          10.17           active  \n",
      "2  Fall 2018       20       NaN          10.29           active  \n",
      "3  Fall 2018       20       NaN          10.30           active  \n",
      "4  Fall 2018       20       NaN          10.32           active  \n",
      "student id             0\n",
      "course id              0\n",
      "course sis          3001\n",
      "section                0\n",
      "section id             0\n",
      "section sis         3001\n",
      "term                   0\n",
      "term id                0\n",
      "term sis            3001\n",
      "current score          0\n",
      "enrollment state       0\n",
      "dtype: int64\n",
      "   course id  unposted final score          login time  student id\n",
      "0        639                   0.0 2018-08-13 10:46:00       38094\n",
      "1        659                   0.0 2018-08-13 11:04:00       22168\n",
      "2        659                   0.0 2018-08-13 03:19:00       37144\n",
      "3        638                   0.0 2018-08-13 09:04:00       38000\n",
      "4        631                   0.0 2018-08-14 01:12:00       38410\n"
     ]
    }
   ],
   "source": [
    "#df2 is the same as Course Grades (Final Grade Only View) Fall 2018 but with a column for student names\n",
    "print(df2.head())\n",
    "print(df2.isna().sum() )\n",
    "login_times_clean = login_times_clean.sort_index()\n",
    "print(login_times_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theses cell will not run out of order from the rest of the cells since they run permanent changes and use references.\n",
    "\n",
    "#Let's clean this right now\n",
    "login_times_clean.columns = ['course id', 'unposted final score',\n",
    "       'login time', 'student id']\n",
    "\n",
    "#first convert the login column into something that is readable to pd.to_datetime\n",
    "for index, entry in enumerate(login_times_clean['login time']):\n",
    "    login_times_clean['login time'][index] = entry[0:-2] + entry[-2:].upper() + ' 2018'\n",
    "    if login_times_clean['login time'][index][0:4] == 'Sept':\n",
    "        login_times_clean['login time'][index] = 'Sep ' + entry[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert login time new into a datetime object\n",
    "time_format = '%b %d at %H:%M%p %Y'\n",
    "login_times_clean['login time'] = pd.to_datetime(login_times_clean['login time'], format=time_format)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 4)\n",
      "course id               0\n",
      "unposted final score    0\n",
      "login time              0\n",
      "student id              0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course id</th>\n",
       "      <th>unposted final score</th>\n",
       "      <th>login time</th>\n",
       "      <th>student id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-13 10:46:00</td>\n",
       "      <td>38094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-13 11:04:00</td>\n",
       "      <td>22168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-13 03:19:00</td>\n",
       "      <td>37144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-13 09:04:00</td>\n",
       "      <td>38000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-14 01:12:00</td>\n",
       "      <td>38410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   course id  unposted final score          login time  student id\n",
       "0        639                   0.0 2018-08-13 10:46:00       38094\n",
       "1        659                   0.0 2018-08-13 11:04:00       22168\n",
       "2        659                   0.0 2018-08-13 03:19:00       37144\n",
       "3        638                   0.0 2018-08-13 09:04:00       38000\n",
       "4        631                   0.0 2018-08-14 01:12:00       38410"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now this data frame is prepped for merging\n",
    "print(login_times_clean.shape)\n",
    "print(login_times_clean.isna().sum())\n",
    "login_times_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     course id  unposted final score          login time  student id  \\\n",
      "0          639                  0.00 2018-08-13 10:46:00       38094   \n",
      "1          659                  0.00 2018-08-13 11:04:00       22168   \n",
      "2          659                  0.00 2018-08-13 03:19:00       37144   \n",
      "3          638                  0.00 2018-08-13 09:04:00       38000   \n",
      "4          631                  0.00 2018-08-14 01:12:00       38410   \n",
      "5          633                  6.21 2018-08-14 01:45:00       38133   \n",
      "6          644                  0.00 2018-08-14 11:12:00       38304   \n",
      "7          659                  0.00 2018-08-14 11:29:00       38320   \n",
      "8          639                  1.69 2018-08-14 11:49:00       38903   \n",
      "9          636                  0.00 2018-08-14 12:12:00       37677   \n",
      "10         631                  6.21 2018-08-14 12:15:00       38299   \n",
      "11         639                  0.00 2018-08-14 12:53:00       21575   \n",
      "12         659                  0.00 2018-08-14 02:47:00       37764   \n",
      "13         658                  0.00 2018-08-14 04:16:00       23026   \n",
      "14         638                  5.65 2018-08-14 09:00:00       37633   \n",
      "15         639                  0.00 2018-08-14 09:42:00       38838   \n",
      "16         631                  1.69 2018-08-15 01:19:00       38130   \n",
      "17         656                  0.00 2018-08-15 01:21:00       39095   \n",
      "18         637                  1.11 2018-08-15 01:43:00       38920   \n",
      "19         658                  4.79 2018-08-15 10:27:00       38018   \n",
      "20         658                  0.00 2018-08-15 11:06:00       39019   \n",
      "21         626                  9.52 2018-08-15 11:15:00       38651   \n",
      "22         655                  1.13 2018-08-15 11:55:00        4605   \n",
      "23         654                  0.00 2018-08-15 06:32:00       39274   \n",
      "24         655                  0.00 2018-08-15 08:48:00       30163   \n",
      "25         653                  0.00 2018-08-16 01:16:00       37738   \n",
      "26         655                  6.21 2018-08-16 01:52:00       37861   \n",
      "27         656                  0.00 2018-08-16 11:33:00       39152   \n",
      "28         633                  0.00 2018-08-16 05:54:00       39635   \n",
      "29         644                  0.00 2018-08-17 11:37:00       39949   \n",
      "..         ...                   ...                 ...         ...   \n",
      "460        655                 27.99 2018-09-24 11:35:00       38064   \n",
      "461        633                  3.28 2018-09-24 07:41:00       41242   \n",
      "462        658                 27.67 2018-09-25 03:13:00       39726   \n",
      "463        655                  7.01 2018-09-26 10:26:00       40311   \n",
      "464        635                 11.30 2018-09-26 12:42:00       37916   \n",
      "465        637                 27.66 2018-09-26 04:47:00       31087   \n",
      "466        659                 17.32 2018-09-26 08:17:00       38223   \n",
      "467        634                 28.07 2018-09-27 01:27:00       39643   \n",
      "468        640                  0.00 2018-09-27 12:05:00       43488   \n",
      "469        639                 24.65 2018-09-27 03:44:00       39141   \n",
      "470        658                 31.18 2018-09-28 02:20:00       38998   \n",
      "471        634                 24.79 2018-09-28 04:29:00       37869   \n",
      "472        634                 18.48 2018-09-28 09:32:00       40677   \n",
      "473        635                 44.69 2018-09-29 10:28:00       37200   \n",
      "474        655                 10.73 2018-09-03 09:59:00       35480   \n",
      "475        644                  0.00 2018-09-30 01:30:00       39966   \n",
      "476        635                 13.45 2018-09-30 02:51:00       38863   \n",
      "477        653                 34.33 2018-09-30 05:12:00       34356   \n",
      "478        643                 22.80 2018-09-30 07:57:00       41858   \n",
      "479        639                  8.93 2018-09-04 01:36:00       37957   \n",
      "480        637                  3.31 2018-09-05 10:27:00       40946   \n",
      "481        638                  8.93 2018-09-05 11:27:00       38397   \n",
      "482        655                  5.65 2018-09-06 10:27:00       22959   \n",
      "483        629                 21.60 2018-09-07 10:58:00       37707   \n",
      "484        632                 20.31 2018-09-07 02:59:00       39518   \n",
      "485        658                  0.00 2018-09-07 03:32:00       30225   \n",
      "486        635                 25.65 2018-09-07 03:59:00       37491   \n",
      "487        644                  5.92 2018-09-08 08:52:00       38337   \n",
      "488        623                  1.08 2018-09-09 10:16:00       34648   \n",
      "489        654                  7.25 2018-09-09 12:58:00       39884   \n",
      "\n",
      "     course sis        section  section id  section sis       term  term id  \\\n",
      "0           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "1           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "2           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "3           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "4           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "5           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "6           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "7           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "8           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "9           NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "10          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "11          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "12          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "13          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "14          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "15          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "16          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "17          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "18          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "19          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "20          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "21          NaN   WELL 1000/08      1165.0          NaN  Fall 2018     20.0   \n",
      "22          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "23          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "24          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "25          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "26          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "27          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "28          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "29          NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "..          ...            ...         ...          ...        ...      ...   \n",
      "460         NaN  WELL 1000/W84      1222.0          NaN  Fall 2018     20.0   \n",
      "461         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "462         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "463         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "464         NaN   WELL1000/W67      1184.0          NaN  Fall 2018     20.0   \n",
      "465         NaN  WELL 1000/W70      1187.0          NaN  Fall 2018     20.0   \n",
      "466         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "467         NaN   WELL 1000/36      1180.0          NaN  Fall 2018     20.0   \n",
      "468         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "469         NaN  WELL 1000/W72      1191.0          NaN  Fall 2018     20.0   \n",
      "470         NaN   WELL 1000/11      1229.0          NaN  Fall 2018     20.0   \n",
      "471         NaN   WELL 1000/36      1180.0          NaN  Fall 2018     20.0   \n",
      "472         NaN   WELL 1000/36      1180.0          NaN  Fall 2018     20.0   \n",
      "473         NaN   WELL1000/W66      1183.0          NaN  Fall 2018     20.0   \n",
      "474         NaN  WELL 1000/W84      1222.0          NaN  Fall 2018     20.0   \n",
      "475         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "476         NaN   WELL1000/W66      1183.0          NaN  Fall 2018     20.0   \n",
      "477         NaN  WELL 1000/W82      1217.0          NaN  Fall 2018     20.0   \n",
      "478         NaN   WELL 1000/17      1200.0          NaN  Fall 2018     20.0   \n",
      "479         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "480         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "481         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "482         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "483         NaN   WELL 1000/13      1172.0          NaN  Fall 2018     20.0   \n",
      "484         NaN   WELL 1000/43      1177.0          NaN  Fall 2018     20.0   \n",
      "485         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "486         NaN   WELL1000/W67      1184.0          NaN  Fall 2018     20.0   \n",
      "487         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "488         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "489         NaN            NaN         NaN          NaN        NaN      NaN   \n",
      "\n",
      "     term sis  current score enrollment state  \n",
      "0         NaN            NaN              NaN  \n",
      "1         NaN            NaN              NaN  \n",
      "2         NaN            NaN              NaN  \n",
      "3         NaN            NaN              NaN  \n",
      "4         NaN            NaN              NaN  \n",
      "5         NaN            NaN              NaN  \n",
      "6         NaN            NaN              NaN  \n",
      "7         NaN            NaN              NaN  \n",
      "8         NaN            NaN              NaN  \n",
      "9         NaN            NaN              NaN  \n",
      "10        NaN            NaN              NaN  \n",
      "11        NaN            NaN              NaN  \n",
      "12        NaN            NaN              NaN  \n",
      "13        NaN            NaN              NaN  \n",
      "14        NaN            NaN              NaN  \n",
      "15        NaN            NaN              NaN  \n",
      "16        NaN            NaN              NaN  \n",
      "17        NaN            NaN              NaN  \n",
      "18        NaN            NaN              NaN  \n",
      "19        NaN            NaN              NaN  \n",
      "20        NaN            NaN              NaN  \n",
      "21        NaN          10.06           active  \n",
      "22        NaN            NaN              NaN  \n",
      "23        NaN            NaN              NaN  \n",
      "24        NaN            NaN              NaN  \n",
      "25        NaN            NaN              NaN  \n",
      "26        NaN            NaN              NaN  \n",
      "27        NaN            NaN              NaN  \n",
      "28        NaN            NaN              NaN  \n",
      "29        NaN            NaN              NaN  \n",
      "..        ...            ...              ...  \n",
      "460       NaN          27.99           active  \n",
      "461       NaN            NaN              NaN  \n",
      "462       NaN            NaN              NaN  \n",
      "463       NaN            NaN              NaN  \n",
      "464       NaN          11.83           active  \n",
      "465       NaN          28.94           active  \n",
      "466       NaN            NaN              NaN  \n",
      "467       NaN          28.07           active  \n",
      "468       NaN            NaN              NaN  \n",
      "469       NaN          24.65           active  \n",
      "470       NaN          31.18           active  \n",
      "471       NaN          24.79           active  \n",
      "472       NaN          18.48           active  \n",
      "473       NaN          46.80           active  \n",
      "474       NaN          10.73           active  \n",
      "475       NaN            NaN              NaN  \n",
      "476       NaN          14.08           active  \n",
      "477       NaN          35.96           active  \n",
      "478       NaN          20.90           active  \n",
      "479       NaN            NaN              NaN  \n",
      "480       NaN            NaN              NaN  \n",
      "481       NaN            NaN              NaN  \n",
      "482       NaN            NaN              NaN  \n",
      "483       NaN          22.60           active  \n",
      "484       NaN          21.89           active  \n",
      "485       NaN            NaN              NaN  \n",
      "486       NaN          26.86           active  \n",
      "487       NaN            NaN              NaN  \n",
      "488       NaN            NaN              NaN  \n",
      "489       NaN            NaN              NaN  \n",
      "\n",
      "[490 rows x 13 columns]\n",
      "course id                 0\n",
      "unposted final score      0\n",
      "login time                0\n",
      "student id                0\n",
      "course sis              490\n",
      "section                 117\n",
      "section id              117\n",
      "section sis             490\n",
      "term                    117\n",
      "term id                 117\n",
      "term sis                490\n",
      "current score           117\n",
      "enrollment state        117\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Merge\n",
    "grades_login_times = pd.merge(login_times_clean, df2, on = ['student id', 'course id'], how = 'left')\n",
    "\n",
    "#Now this is EDA ready\n",
    "print(grades_login_times)\n",
    "print(grades_login_times.isna().sum())\n",
    "#grades_login_times.to_csv('grades_and_login_times(below 70).csv')\n",
    "\n",
    "#why are there so many NaN values even though each dataframe has no missing values and I think they originally came \n",
    "#from the same dataframe...?\n",
    "#Shit it looks like there are student ids that are not in df2 along with student ids that do not match their course id\n",
    "#in the dataframe, have to figure out who those students are or something. I don't want to deal with that shit right\n",
    "#now though. I should do some EDA on the other dataframe and then come back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 13)\n",
      "(119, 13)\n"
     ]
    }
   ],
   "source": [
    "#Find the students who stopped being active before the drop date Aug 19\n",
    "drop_date = pd.Timestamp(datetime.date(2018, 8, 20))\n",
    "before_aug = grades_login_times[grades_login_times['login time'] <= drop_date]\n",
    "print(before_aug.shape)\n",
    "\n",
    "#Find the students who stopped being before the other date thing, Oct 3\n",
    "other_date = pd.Timestamp(datetime.date(2018, 10, 4))\n",
    "before_oct = grades_login_times[(grades_login_times['login time'] >= drop_date) & (grades_login_times['login time'] <= other_date)]\n",
    "print(before_oct.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOther Data sets to consider are Click History and Assessment Responses\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Other Data sets to consider are Click History and Assessment Responses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
