{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis could be markdown later. Include something about the note book in here if necessary.\\n'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This could be markdown later. Include something about the note book in here if necessary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangling Section 1: Assessment Page Views.csv\n",
    "\"\"\"\n",
    "#Read in csv as dataframe\n",
    "#Using a smaller subset of the data since testing with 6mil+ rows would be too intensive\n",
    "df = pd.read_csv('/Users/ethanrowe/Dev/springboard_data/KSU Assessment Page Views Fall 2018.csv', nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate Outliers using the 1.5*interquartile rule\n",
    "IQR = float(df['duration'].quantile([0.75])) - float(df['duration'].quantile([0.25]))\n",
    "upper_bound = float(df['duration'].quantile([0.75])) + 1.5*IQR\n",
    "lower_bound = float(df['duration'].quantile([0.25])) - 1.5*IQR\n",
    "\n",
    "df_clean = df[(df['duration'] < upper_bound) & (df['duration'] > lower_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data:\n",
      "(8846, 13)\n",
      "Null Values: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned Data:\")\n",
    "print(df_clean.shape)\n",
    "print(\"Null Values: \" + str(df_clean.isnull().values.any()))\n",
    "#df_clean.to_csv('KSU Assessment Page Views Fall 2018 Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine if there are missing values missing values\n",
    "total =  []\n",
    "for chunk in pd.read_csv('/Users/ethanrowe/Dev/springboard_data/KSU Assessment Page Views Fall 2018.csv', chunksize = 1000):\n",
    "    if chunk.isnull().values.any():\n",
    "        total.append(chunk)\n",
    "        \n",
    "#There are missing values in the whole dataframe\n",
    "#Need to check user_id, canvas_assignment_d, canvas_attempt_id, page_ids\n",
    "#There are no missing values in the columns that we are taking an interest in for any of our calculations\n",
    "#Checked each column for missing values and each column had none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grouped by Assignment\n",
      "(580, 1)\n",
      "\n",
      "Grouped by User\n",
      "(1865, 1)\n"
     ]
    }
   ],
   "source": [
    "#Group by page and by students to compare average assignment durations\n",
    "assignment_df = pd.DataFrame(df_clean.groupby(['canvas_assignment_id'])['duration'].mean())\n",
    "print('\\nGrouped by Assignment')\n",
    "assignment_df.columns = ['Average Duration']\n",
    "print(page_df.shape)\n",
    "\n",
    "user_df = pd.DataFrame(df_clean.groupby(['user_param_external_user_id', 'canvas_assignment_id'])['duration'].mean())\n",
    "print('\\nGrouped by User')\n",
    "user_df.columns = ['Average Duration']\n",
    "print(user_df.shape)\n",
    "\n",
    "\n",
    "#page_df.to_csv('KSU Assessment Assignment Duration Averages.csv')\n",
    "#user_df.to_csv('KSU Assessment Student Views by User.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#User_df is good for visualizing the data, but a df that is easier to work with will be labeled sub_df. It has the \n",
    "#same columns as user_df, just in a different order with a numerical index. It will also be sorted by \n",
    "#canvas_assignment_id in order to match assignment_df's index\n",
    "sub_df = user_df.reset_index()  \n",
    "sub_df = sub_df.set_index('canvas_assignment_id')  \n",
    "sub_df = sub_df.sort_index()     \n",
    "sub_df = sub_df.reset_index()    \n",
    "\n",
    "#Using the user_df, turn it back into a dictionary grouped by canvas_assignment ids\n",
    "dfs = {}       \n",
    "for entry in sub_df.iterrows():  \n",
    "    if index != entry[1]['canvas_assignment_id']:  \n",
    "        index = entry[1]['canvas_assignment_id']  \n",
    "        df = pd.DataFrame(columns=['canvas_assignment_id', 'user_param_external_user_id', 'Average Duration'])  \n",
    "        df = df.append(dict(entry[1]), ignore_index = True) \n",
    "        dfs[index] = df  \n",
    "    else:  \n",
    "        dfs[index] = dfs[index].append(dict(entry[1]), ignore_index = True)  \n",
    "\n",
    "#Now page_df and this dictionary dfs should have one to one matches for the loop below\n",
    "#This loop will find all the student-assignment_id pairs that had an above average duration for that assignment\n",
    "above_avg_df = pd.DataFrame(columns = ['canvas_assignment_id', 'user_param_external_user_id','Average Duration']) \n",
    "for entry in assignment_df.iterrows(): \n",
    "    df = dfs[entry[0]] \n",
    "    above_avg_chunk = df[df['Average Duration'] >= entry[1][0]] \n",
    "    above_avg_df = above_avg_df.append(above_avg_chunk) \n",
    "#above_avg_df.to_csv('KSU Students with Above Average Assignment Duration.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students with longer than average assignmnet durations:\n",
      "(1018, 3)\n",
      "   canvas_assignment_id  user_param_external_user_id  Average Duration\n",
      "0               40876.0                      37827.0      12831.500000\n",
      "0               40878.0                      37949.0       9549.583333\n",
      "3               40878.0                      37251.0      10307.500000\n",
      "0               40879.0                      38399.0      11579.000000\n",
      "5               40879.0                      40394.0      16064.857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Students with longer than average assignmnet durations:\")\n",
    "print(above_avg_df.shape)\n",
    "print(above_avg_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangling Section 2: Course Grades \n",
    "Course Grades has been cleand by hand in excel. The details are outlined in Data Wrangling.md\n",
    "\"\"\"\n",
    "#Import data\n",
    "df2 = pd.read_csv('/Users/ethanrowe/Dev/springboard_data/Course Grades (Final Grade Only View) Fall 2018 - Course Grades (Final Grade Only View) Fall 2018.csv')\n",
    "login_times_clean = pd.read_csv('/Users/ethanrowe/Dev/springboard_data/anon_students_login_times_clean.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTo-Do\\nLoad in correct csv's, double check what I need for this\\nMerge them or something like that or augment login_times_clean with the necessary columns\\n\\nDo stuff on df2 that is necessary for cleaning like checking for missing values.\\n\\nDo stuff on login_times_clean such as breaking it up by times and counting stuff. Basically all of this has been \\ndone in excel already, but this will be more accurate and adaptable moving forward.\\n\\n\""
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The next step will be to merge login_times_clean and df2. df2 has all of the grade \n",
    "#information for certain students by name and id\n",
    "\n",
    "#I might need to use a different dataset since that one is not anonymous, check Course Final Grades or something later\n",
    "#login_times_clean is also not anonymous and it has the last access time for students by name and id\n",
    "\n",
    "#I want a dataframe that has students grades along with last access times. Then I can break that up into those who\n",
    "#were active before and after certain periods to get a good idea of the distribution.\n",
    "\n",
    "#All of the students in login_times_clean have final grades less than 70%. I can use df2 for a number of things on its\n",
    "#own but I also need it to put login_times_clean into perspective.\n",
    "\n",
    "#First step is to clean df2 if necessary. Then start prepping all of them. They should be EDA ready by the end of this.\n",
    "\n",
    "\"\"\"\n",
    "To-Do\n",
    "Load in correct csv's, double check what I need for this\n",
    "Merge them or something like that or augment login_times_clean with the necessary columns\n",
    "\n",
    "Do stuff on df2 that is necessary for cleaning like checking for missing values.\n",
    "\n",
    "Do stuff on login_times_clean such as breaking it up by times and counting stuff. Basically all of this has been \n",
    "done in excel already, but this will be more accurate and adaptable moving forward.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   student id  course id  course sis        section  section id  section sis  \\\n",
      "0       38651        626         NaN   WELL 1000/08        1165          NaN   \n",
      "1       37597        655         NaN  WELL 1000/W83        1221          NaN   \n",
      "2       35615        646         NaN   WELL 1000/38        1205          NaN   \n",
      "3       37982        633         NaN  WELL 1000/W65        1179          NaN   \n",
      "4       40291        623         NaN   WELL 1000/07        1164          NaN   \n",
      "\n",
      "        term  term id  term sis  current score enrollment state  \n",
      "0  Fall 2018       20       NaN          10.06           active  \n",
      "1  Fall 2018       20       NaN          10.17           active  \n",
      "2  Fall 2018       20       NaN          10.29           active  \n",
      "3  Fall 2018       20       NaN          10.30           active  \n",
      "4  Fall 2018       20       NaN          10.32           active  \n",
      "   Course Id  Unposted Final Scores         Login Time  student id\n",
      "0        639                    0.0  Aug 13 at 10:46pm       38094\n",
      "1        659                    0.0  Aug 13 at 11:04am       22168\n",
      "2        659                    0.0   Aug 13 at 3:19pm       37144\n",
      "3        638                    0.0   Aug 13 at 9:04pm       38000\n",
      "4        631                    0.0   Aug 14 at 1:12pm       38410\n"
     ]
    }
   ],
   "source": [
    "#df2 is the same as Course Grades (Final Grade Only View) Fall 2018 but with a column for student names\n",
    "print(df2.head())\n",
    "\n",
    "login_times_clean = login_times_clean.sort_index()\n",
    "print(login_times_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theses cell will not run out of order from the rest of the cells since they run permanent changes and use references.\n",
    "\n",
    "#Let's clean this right now\n",
    "login_times_clean.columns = ['course id', 'unposted final score',\n",
    "       'login time', 'student id']\n",
    "\n",
    "#first convert the login column into something that is readable to pd.to_datetime\n",
    "for index, entry in enumerate(login_times_clean['login time']):\n",
    "    login_times_clean['login time'][index] = entry[0:-2] + entry[-2:].upper() + ' 2018'\n",
    "    if login_times_clean['login time'][index][0:4] == 'Sept':\n",
    "        login_times_clean['login time'][index] = 'Sep ' + entry[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert login time new into a datetime object\n",
    "time_format = '%b %d at %H:%M%p %Y'\n",
    "login_times_clean['login time'] = pd.to_datetime(login_times_clean['login time'], format=time_format)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course id</th>\n",
       "      <th>unposted final score</th>\n",
       "      <th>login time</th>\n",
       "      <th>student id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-13 10:46:00</td>\n",
       "      <td>38094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-13 11:04:00</td>\n",
       "      <td>22168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-13 03:19:00</td>\n",
       "      <td>37144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-13 09:04:00</td>\n",
       "      <td>38000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-08-14 01:12:00</td>\n",
       "      <td>38410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   course id  unposted final score          login time  student id\n",
       "0        639                   0.0 2018-08-13 10:46:00       38094\n",
       "1        659                   0.0 2018-08-13 11:04:00       22168\n",
       "2        659                   0.0 2018-08-13 03:19:00       37144\n",
       "3        638                   0.0 2018-08-13 09:04:00       38000\n",
       "4        631                   0.0 2018-08-14 01:12:00       38410"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now this data frame is prepped for merging\n",
    "print(login_times_clean.shape)\n",
    "login_times_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Merge\n",
    "grades_login_times = pd.merge(login_times_clean, df2, on = ['student id', 'course id'], how = 'left')\n",
    "\n",
    "#Now this is EDA ready\n",
    "grades_login_times\n",
    "grades_login_times.to_csv('grades_and_login_times(below 70).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOther Data sets to consider are Click History and Assessment Responses\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Other Data sets to consider are Click History and Assessment Responses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
